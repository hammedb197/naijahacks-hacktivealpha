{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# -1 represent  phishing url while 1 represent not phishing url, returning zero means Suspicious\n",
    "#length of urls\n",
    "\n",
    "#if url contains ip addresses instead of name http://125.98.3.123/fake.html\n",
    "def ip_address(url):\n",
    "    match=re.search('[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}'\n",
    "                    '(?:^|(?<=\\s))(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))(?=\\s|$)'\n",
    "                    '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',url)\n",
    "    if match:\n",
    "        return -1  \n",
    "    else:\n",
    "        return 1\n",
    "#print(ip_address('http://125.98.3.123/fake.html'))\n",
    "\n",
    "def url_length(url):\n",
    "    if len(url)<54:\n",
    "        return 1\n",
    "    elif len(url)>=54|len(url)<=75:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    \n",
    "#if url contains shortening services something like bit.ly/19DXSk4\n",
    "def url_shortener(url):\n",
    "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "# Using “@” symbol in the URL leads the browser to ignore everything preceding the “@” symbol and the real address often follows the “@” symbol. \n",
    "def url_symbol(url):\n",
    "    match=re.search(\"@\", url)\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "def url_redirect(url):\n",
    "    list = [x.start(0) for x in re.finditer('\\\\.',url)]\n",
    "    if list[len(list)-1]>6:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "#print(url_redirect('https://www.legitimate.com//http://www.phishing.com'))\n",
    "   \n",
    "# Adding Prefix or Suffix Separated by (-) to the Domain\n",
    "def pref_suf(url):\n",
    "    match = re.search(\"-\", url)\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "# Sub Domain and Multi Sub Domains\n",
    "def sub_domain(url):\n",
    "     if(ip_address(url)==-1):\n",
    "        match = re.search('[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}'\n",
    "                    '(?:^|(?<=\\s))(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))(?=\\s|$)'\n",
    "                    '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',url)\n",
    "        pos = match.end(0)\n",
    "        url = url[pos:]\n",
    "        list = [x.start(0) for x in re.finditer('\\.',url)]\n",
    "        if len(list)<=3:\n",
    "            return 1\n",
    "        elif len(list) == 4:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "#def submitting_to_mail(url):\n",
    "    #web = urlopen(url)\n",
    "    #soup = bs(web, 'html.parser')\n",
    "    #for form in soup.findAll('form', action= True):\n",
    "      #  if \"mailto:\" in form['action'] :\n",
    "     #       return -1\n",
    "    #     else:\n",
    " #           return 1\n",
    "#print(submitting_to_mail(\"https://whogohost.com\"))\n",
    "# Website Traffic \n",
    "\n",
    "def web_traffic(url):\n",
    "    content = urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url)\n",
    "    try:\n",
    "        rank = bs(content, \"xml\").find(\"REACH\")['RANK']\n",
    "        rank= int(rank)\n",
    "    except Exception as e:\n",
    "        return -1\n",
    "    if (rank<100000):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "   \n",
    "#print(web_traffic(\"facebook.com\"))\n",
    "\n",
    "def main(url= 'www.iralan.com'):\n",
    "    hostname = url\n",
    "    h = [(x.start(0), x.end(0)) for x in re.finditer('https://|http://|www.|https://www.|http://www.', hostname)]\n",
    "    print(h)\n",
    "    ulen = int(len(h))\n",
    "    if ulen != 0:\n",
    "        y = h[0][1]\n",
    "        hostname = hostname[y:]\n",
    "    features_extract = []\n",
    "    #features_extract.append(ip_address(url))\n",
    "    features_extract.append(url_length(url))\n",
    "    features_extract.append(url_shortener(url))\n",
    "    features_extract.append(url_symbol(url))\n",
    "    features_extract.append(url_redirect(url))\n",
    "    features_extract.append(pref_suf(url))\n",
    "    features_extract.append(sub_domain(url))\n",
    "    features_extract.append(web_traffic(url))\n",
    "    \n",
    "    return features_extract\n",
    "         \n",
    "    if __name__ == \"__main__\":\n",
    "        main(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
